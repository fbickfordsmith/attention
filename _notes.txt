----------
PLAN

7 aug
[x] redefine semcontexts (Ken's update)
[x] redefine sizecontexts (2**x up to 256)
[x] generate dataframes
[x] train sizecontexts (GPU3, 26hr)

8 aug
[ ] redefine diffcontexts
[ ] redefine simcontexts
[ ] generate dataframes

when others finished
[ ] train semcontexts (GPU0, 17hr)
[ ] test semcontexts (GPU0, 30min)
[ ] test sizecontexts (GPU3, 30min)
[ ] train diffcontexts (GPU0, 63hr)
[ ] train simcontexts (GPU3, 50hr)
[ ] test diffcontexts (GPU0, 1hr)
[ ] test simcontexts (GPU3, 1hr)

if there's time
- {diff, size, sim}-controlled semantic
- repeats (eg, of size experiment)
- speed up generator
- look into processing imagenet hierarchy
- variational inference
- interpretability

----------
NOTES TO WRITE UP

using diagonal covariance,
- mahalanobis_v3.py takes 20s/loop * 1000loops = 5.5hr
- kl_divergence_v2.py takes 40s/loop * 1000loops = 11hr
- similarity_dotproduct_np2.py with num_samples=125 takes 17s/loop * 1000loops = 4.5hr
distances are (much) slower to compute with full covariance

perceptual similarity on val_white
- vgg takes 30s/loop * 500,500loops = 4170hr
- alexnet takes 20s/loop
- squeezenet takes 30s/loop

----------
THESIS

write without looking at references -- just type!
overall difficulty -> average difficulty

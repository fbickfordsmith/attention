----------
NEXT STEPS

- split up activations
- time a forward pass with precomputed conv5 output

----------
CODE REMINDERS

- get updated semcontext definitions
- revert attention_position
- revert losses and metrics
- revert _v6.npy
- revert data_aug

----------
NOTES TO WRITE UP

- repeat semcontexts with data augmentation
- try perceptual similarity model
- get flow_from_dataframe working
- repeat sizecontexts with different contexts
- try similarity measures with full covariances

similarity measures
- ssim: scikit-image, tensorflow etc
- kl, sym kl, js, mahalanobis, frechet/wasserstein-2: tensorflow etc

full covariance matrix takes too long to find, and distances are slower to
compute with full covariance. can store as 32-bit floats to reduce storage
required.

mahalanobis_v3.py takes 20s/loop * 1000loops = 5.5hr
kl_divergence_v2.py takes 40s/loop * 1000loops = 11hr
similarity_dotproduct_np2.py with num_samples=125 takes 17s/loop * 1000loops = 4.5hr

perceptual similarity on val_white
- vgg takes 30s/loop * 500,500loops = 4170hr
- alexnet takes 20s/loop
- squeezenet takes 30s/loop

contexts_testing takes 3 mins per context

----------
PLAN

7 aug
[x] redefine semcontexts (Ken's update)
[x] redefine sizecontexts (2**x up to 256)
[x] generate semcontexts, sizecontexts dataframes
[x] train sizecontexts (took ~24hr)

8-9 aug
[x] test sizecontexts
[x] train semcontexts (took ~12hr)
[ ] test semcontexts
[x] redefine diffcontexts (extra 5 covering acc in [0.2, 0.4])
[x] redefine simcontexts (reduce to 20; minimising std_acc)
[ ] generate dataframes
[ ] train diffcontexts

when others finished

[ ] train diffcontexts (GPU0, 63hr)
[ ] train simcontexts (GPU3, 50hr)
[ ] test diffcontexts (GPU0, 1hr)
[ ] test simcontexts (GPU3, 1hr)

if there's time
- redefine sizecontexts with unusual acc or sim
- {diff, size, sim}-controlled semantic
- repeats (eg, of size experiment)
- speed up generator
- look into processing imagenet hierarchy
- variational inference
- interpretability

----------
NOTES TO WRITE UP

using diagonal covariance,
- mahalanobis_v3.py takes 20s/loop * 1000loops = 5.5hr
- kl_divergence_v2.py takes 40s/loop * 1000loops = 11hr
- similarity_dotproduct_np2.py with num_samples=125 takes 17s/loop * 1000loops = 4.5hr
distances are (much) slower to compute with full covariance

perceptual similarity on val_white
- vgg takes 30s/loop * 500,500loops = 4170hr
- alexnet takes 20s/loop
- squeezenet takes 30s/loop

need to correct all instances of mean_dist computations

----------
THESIS

write without looking at references -- just type!
overall difficulty -> average difficulty

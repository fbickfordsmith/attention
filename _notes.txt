----------
TO DO

tasks:
- repeat semcontexts with data augmentation
- try perceptual similarity model
- get flow_from_dataframe working
- repeat sizecontexts with different contexts
- try similarity measures with full covariances

code:
- patience=1 -> patience=10
- flow_from_directory -> flow_from_dataframe
- simplify context csv files
- combine npy files in results/
- tidy keras-models/
- save activations at output of VGG1 (need to assess I/O vs forward pass)

----------
NOTES

similarity measures
- ssim: scikit-image, tensorflow etc
- kl, sym kl, js, mahalanobis, frechet/wasserstein-2: tensorflow etc

remember to change back `datagen_train` in `training.py`

full covariance matrix takes too long to find, and distances are slower to
compute with full covariance. can store as 32-bit floats to reduce storage
required.

mahalanobis_v3.py takes 20s/loop * 1000loops = 5.5hr
kl_divergence_v2.py takes 40s/loop * 1000loops = 11hr
similarity_dotproduct_np2.py with num_samples=125 takes 17s/loop * 1000loops = 4.5hr

perceptual similarity on val_white
- vgg takes 30s/loop * 500,500loops = 4170hr
- alexnet takes 20s/loop
- squeezenet takes 30s/loop
